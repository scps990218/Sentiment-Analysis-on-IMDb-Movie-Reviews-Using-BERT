# -*- coding: utf-8 -*-
"""HW2-BERT-110421072.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ifQKTRbwm28bDqBmfXppb-uaslVKzw_r
"""

!pip install datasets transformers

!pip install datasets transformers torchmetrics

import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel
from sklearn.model_selection import train_test_split
import torch
import torch.nn.functional as Fun
import transformers
import matplotlib.pyplot as plt
import pandas as pd
import time
import warnings
warnings.filterwarnings('ignore') # setting ignore as a parameter

from sklearn.metrics import confusion_matrix

# get predict result
def get_pred(logits):
  return logits.argmax(dim=-1)


# calculate confusion metrics
def cal_metrics(pred, ans):
  tp, fp, fn, tn = confusion_matrix(pred, ans,labels=[1,0]).ravel()
  accuracy = (tp + tn) / (tp + fp + fn + tn)
  precision = tp / (tp + fp)
  recall = tp / (tp + fn)
  f1score = 2 / ((1 / precision) + (1 / recall))
  return accuracy, precision, recall, f1score

# save model to path
def save_checkpoint(save_path, model):
  if save_path == None:
      return
  torch.save(model.state_dict(), save_path)
  print(f'Model saved to ==> {save_path}')

# load model from path
def load_checkpoint(load_path, model, device):    
  if load_path==None:
      return
  state_dict = torch.load(load_path, map_location=device)
  print(f'Model loaded from <== {load_path}')
  
  model.load_state_dict(state_dict)
  return model

"""#載入資料

合併train和test的資料後切割成8/1/1的train/val/test資料集
"""

from datasets import load_dataset
#https://huggingface.co/datasets/imdb/viewer/plain_text/train
dataset = load_dataset("imdb")

dataset

dataset['train'][0]

import pandas as pd

train_dataset = dataset.get("train")
test_dataset = dataset.get("test")

train_data = [(i["text"], i["label"]) for i in train_dataset]
test_data = [(i["text"], i["label"]) for i in test_dataset]

all_data = train_data + test_data# a list to save all data
all_df = pd.DataFrame(all_data)
all_df = all_df.rename(columns={0: "text", 1: "label"})
all_df

all_df.label.value_counts() / len(all_df)

from sklearn.model_selection import train_test_split #切割train/val/test

train_df, temp_data = train_test_split(all_df, random_state=1111, train_size=0.8)
dev_df, test_df = train_test_split(temp_data, random_state=1111, train_size=0.5)
print('# of train_df:', len(train_df))
print('# of dev_df:', len(dev_df))
print('# of test_df data:', len(test_df))

# save data
train_df.to_csv('./train.tsv', sep='\t', index=False)
dev_df.to_csv('./val.tsv', sep='\t', index=False)
test_df.to_csv('./test.tsv', sep='\t', index=False)

"""#自定義 Dataset，將tokenzie的步驟放進去"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import torch
import torch.nn.functional as Fun

# Using Dataset to build DataLoader
class CustomDataset(Dataset):
  def __init__(self, mode, df, specify, args):
    assert mode in ["train", "val", "test"]  # 一般會切三份
    self.mode = mode
    self.df = df
    self.specify = specify # specify column of data (the column U use for predict)
    if self.mode != 'test':
      self.label = df['label']
    self.tokenizer = AutoTokenizer.from_pretrained(args["config"])
    self.max_len = args["max_len"]
    self.num_class = args["num_class"]
      
  def __len__(self):
    return len(self.df)

  # transform label to one_hot label (if num_class > 2)
  def one_hot_label(self, label):
    return Fun.one_hot(torch.tensor(label), num_classes = self.num_class)
  
  # transform text to its number
  def tokenize(self,input_text):
    data = {}
    inputs = self.tokenizer.encode_plus(
            input_text, 
            None,
            add_special_tokens=True,
            max_length=self.max_len, 
            padding="max_length",
            truncation=True,
            return_token_type_ids=True
          )

    data["input_ids"] = torch.tensor(inputs.input_ids, dtype=torch.long)
    data["attention_mask"] = torch.tensor(inputs.attention_mask, dtype=torch.long)
    data["token_type_ids"] = torch.tensor(inputs.token_type_ids, dtype=torch.long)
    
    return data["input_ids"], data["attention_mask"],  data["token_type_ids"]

  # get single data
  def __getitem__(self, index):
      
    sentence = str(self.df[self.specify][index])
    ids, mask, token_type_ids = self.tokenize(sentence)
    

    if self.mode == "test":
        return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long)
    else:
        if self.num_class > 2:
          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long), self.one_hot_label(self.label[index])
        else:
          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \
            torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label[index], dtype=torch.long)

"""#建立模型"""

# BERT Model
from torch import nn
class BertClassifier(BertPreTrainedModel):
  def __init__(self, config, args):
    super(BertClassifier, self).__init__(config)
    self.bert = BertModel(config)
    self.dropout = nn.Dropout(parameters['dropout'])
    self.fc1 = nn.Linear(768, parameters['num_class'])
    self.init_weights()

  # forward function, data in model will do this
  def forward(self, input_ids, attention_mask, token_type_ids):
    output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None)
    x = self.dropout(output.pooler_output)
    x = self.fc1(x)
    return x

# evaluate dataloader
def evaluate(model, data_loader, device):
  val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0
  step_count = 0
  loss_fct = torch.nn.CrossEntropyLoss()
  model.eval()
  with torch.no_grad():
    for data in data_loader:
      ids, masks, token_type_ids, labels = [t.to(device) for t in data]

      logits = model(input_ids = ids, 
              token_type_ids = token_type_ids, 
              attention_mask = masks)
      logits = logits.cpu()
      labels = labels.cpu()
      acc, prec, rec, f1 = cal_metrics(get_pred(logits), labels)
      loss = loss_fct(logits, labels)

      print(loss.item())
      val_loss += loss.item()
      val_acc += acc
      val_f1 += f1
      val_rec += rec
      val_prec += prec
      step_count+=1

    val_loss = val_loss / step_count
    val_acc = val_acc / step_count
    val_f1 = val_f1 / step_count
    val_rec = val_rec / step_count
    val_prec = val_prec / step_count
      
  return val_loss, val_acc, val_f1, val_rec, val_prec

"""#開始訓練

### Define Hyperparameters
"""

from datetime import datetime
parameters = {
    "num_class": 2,
    "time": str(datetime.now()).replace(" ", "_"),
    # Hyperparameters
    "model_name": 'BERT',
    "config": 'bert-base-uncased',
    "learning_rate": 1e-4,
    "epochs": 3,
    "max_len": 512,
    "batch_size": 16,
    "dropout": 0.3,
}

"""###載入資料"""

import transformers
import pandas as pd

# load training data
train_df = pd.read_csv('/content/drive/MyDrive/ML_Homework/train.tsv', sep = '\t').sample(4000).reset_index(drop=True) 
train_dataset = CustomDataset('train', train_df, 'text', parameters)
train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)

# load validation data
val_df = pd.read_csv('/content/drive/MyDrive/ML_Homework/val.tsv', sep = '\t').sample(500).reset_index(drop=True) 
val_dataset = CustomDataset('val', val_df, 'text', parameters)
val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)

"""###初始化模型"""

import torch.nn as nn

transformers.logging.set_verbosity_error() # close the warning message

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device)
loss_fct = torch.nn.CrossEntropyLoss() # we use cross entrophy loss

## You can custom your optimizer (e.g. SGD .etc) ##
# we use Adam here
optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)

## You also can add your custom scheduler ##
# num_train_steps = len(train_loader) * parameters['epochs]
# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * num_train_steps), num_training_steps=num_train_steps, num_cycles=1)

# Start training
import time
metrics = ['loss', 'acc', 'f1', 'rec', 'prec']
mode = ['train_', 'val_']
record = {s+m :[] for s in mode for m in metrics}

loss_fct = torch.nn.CrossEntropyLoss()


for epoch in range(parameters["epochs"]):

    st_time = time.time()
    train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0
    step_count = 0
    model.train()
    for data in train_loader:

        input_ids = data[0].to(device)
        attention_mask = data[1].to(device)
        token_type_ids = data[2].to(device)
        labels = data[3]    
        optimizer.zero_grad()

        outputs = model(
                input_ids=input_ids, 
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                )
        
        outputs = outputs.cpu()
        acc, prec, rec, f1 = cal_metrics(get_pred(outputs), labels)
        loss = loss_fct(outputs, labels)   
        loss.backward()
        optimizer.step()

        print("loss.item():",loss.item())
        print(acc, prec, rec, f1)
        train_loss += loss.item()
        train_acc += acc
        train_f1 += f1
        train_rec += rec
        train_prec += prec
        step_count += 1

    # evaluate the model performace on val data after finishing an epoch training 
    val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)

    train_loss = train_loss / step_count
    train_acc = train_acc / step_count
    train_f1 = train_f1 / step_count
    train_rec = train_rec / step_count
    train_prec = train_prec / step_count

    print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))
    print('         loss     acc     f1      rec    prec')
    print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))
    print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))

    # record training metrics of each training epoch
    record['train_loss'].append(train_loss)
    record['train_acc'].append(train_acc)
    record['train_f1'].append(train_f1)
    record['train_rec'].append(train_rec)
    record['train_prec'].append(train_prec)

    record['val_loss'].append(val_loss)
    record['val_acc'].append(val_acc)
    record['val_f1'].append(val_f1)
    record['val_rec'].append(val_rec)
    record['val_prec'].append(val_prec)

# save model
save_checkpoint('./bert.pt' , model)

"""#畫圖"""

# draw learning curve
import matplotlib.pyplot as plt
def draw_pics(record, name, img_save=False, show=False):
    x_ticks = range(1, parameters['epochs']+1)
    
    plt.figure(figsize=(6, 3))

    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue', 
             markeredgecolor="teal", markersize=3, markeredgewidth=1, label = 'Train')
    plt.plot(x_ticks, record['val_'+name], '-o', color='pink', 
             markeredgecolor="salmon", markersize=3, markeredgewidth=1, label = 'Val')
    plt.grid(color='lightgray', linestyle='--', linewidth=1)
    
    plt.title('Model', fontsize=14)
    plt.ylabel(name, fontsize=12)
    plt.xlabel('Epoch', fontsize=12)
    plt.xticks(x_ticks, fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')
    
    if img_save:
        plt.savefig(name+'.png', transparent=False, dpi=300)
    if show:
        plt.show()
        
    plt.close()

draw_pics(record, 'loss', img_save=False, show=True)

draw_pics(record, 'acc', img_save=False, show=True)

draw_pics(record, 'f1', img_save=False, show=True)

draw_pics(record, 'rec', img_save=False, show=True)

draw_pics(record, 'prec', img_save=False, show=True)

"""## 預測結果"""

def Softmax(x):
  return torch.exp(x) / torch.exp(x).sum()
# label to class
def label2class(label):
  l2c = {0:'negative', 1:'positive'}
  return l2c[label]

def predict_one(text, model):
    model.eval()
    ids, mask, token_type_ids = tokenizer(text, padding='max_length', truncation=True, max_length=parameters["max_len"], return_tensors="pt").values()
    with torch.no_grad():
        output = model(ids.to(device), attention_mask=mask.to(device), token_type_ids=token_type_ids.to(device))
        probs = torch.softmax(output, dim=1)
        pred = torch.argmax(probs, dim=1)
    return probs.detach().cpu().numpy()[0], pred.item()

# you can load model from existing result
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
init_model = BertClassifier.from_pretrained(parameters['config'], parameters) # build an initial model
model = load_checkpoint('./bert.pt', init_model, device).to(device) # and load the weight of model from specify file

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = AutoTokenizer.from_pretrained(parameters["config"])
# probs, pred = predict_one("This movie doesn't attract me", model)
# print(label2class(pred))

# predict dataloader
def predict(data_loader, model):

  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  total_probs, total_pred = [], []
  model.eval()
  with torch.no_grad():
    for data in data_loader:
      input_ids, attention_mask, \
      token_type_ids = [t.to(device) for t in data]

      # forward pass
      logits = model(input_ids, attention_mask, token_type_ids)
      probs = Softmax(logits) # get each class-probs
      label_index = torch.argmax(probs[0], dim=0)
      pred = label_index.item()

      total_probs.append(probs)
      total_pred.append(pred)

  return total_probs, total_pred

# load testing data
test_df = pd.read_csv('./test.tsv', sep = '\t')
test_dataset = CustomDataset('test', test_df, 'text', parameters)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

total_probs, total_pred = predict(test_loader, model)
res = test_df.copy()
# add predict class of origin file 
res['pred'] = total_pred

# save result
res.to_csv('./result.tsv', sep='\t', index=False)

res.head(5)

correct = 0
for idx, pred in enumerate(res['pred']):
  if pred == res['label'][idx]:
    correct += 1
print('test accuracy = %.4f'%(correct/len(test_df)))